활성화 함수(Activation Function)
활성화되는 조건을 따지는 함수
전기적 신호와 가중치가 모두 더해져 활성화가 될 때 사용되는 함수

1. Step Function(계단 함수)
0에서 0.99까지는 아무 신호가 안나가다가 1이 되었을 때 신호가 나가는
신호가 극단적으로 나가는 변별력을 가지고 있음.
평상시에는 거의 사용을 안하는 방식이다.


2. Sigmoid Function
이진분류(Binary Classfication)에 주로 사용되고
마지막 출력층의 활성화 함수로 사용된다.
y축에 시그모이드를 통과한 x값이 출력된다.
x값이 0 근처를 기반으로 올라간다.

신경망에서 sigmoid function을 중간층에 사용하게되면
중간에 값이 안넘어가다가 갑자기 훅 넘어가는 현상이 생김
결과를 분명하게 만들어주는건 장점이지만
중간층에 사용하게되면 원래 데이터가 망가질 수 있어 중간층에 사용하지않음.


3. ReLU(Rectified Linear Unit)
음의 값이 필요 없을 때 사용, 가장 많이 쓰이는 함수 중 하나.
0 아랫값은 0, 0 넘어가는 값은 그대로 출력

4. 항등 함수(Identity Function)
들어온 값 그대로 내보내는 함수, 함수가 없는 거랑 똑같음
회귀 문제에서 주로 사용함(값이 왜곡되면 안되기 때문에)

5. Softmax
다중 클래스 분류(Multi Class Classfication)에 사용
(동시에 여러개를 분류해야되는 문제에 사용. ex) 사람들의 행동에 따른 성격분류)
입력값의 영향을 크게 받고, 입력값이 크면 출력값도 커진다. 출력값의 총합은 1이다.

6. LeakyReLU
ReLU와 다르게 0 아래 값들의 특성을 조금이라도 확실하게 나타내기 위해 사용

7. ELU(Exponential Linear Units)
0 부분에서 값이 조금 완만하게 올라가는 형태의 그래프가 나타난다.

활성화 함수를 일반적인 사람들이 사용하는 순서
ELU -> LeakyReLU -> ReLU -> tanh -> sigmoid 순
스탠포드 강의에서 언급한 사용 순서
ReLU -> ReLU Family(LeakyReLU, ELU) -> Sigmoid 사용X

Keras
CPU나 GPU를 사용하기 용이해졌다.
AI 접근성과 민주화(특정 계층의 것을 일반 사람들에게 나누어주는 의미)
딥러닝의 대중화
파이썬으로 구현된 high-level 딥러닝 API(추상화 수준이 높다)
      * 추상화 수준
      ex) 하늘에서 눈이 오는데 어쩌구(낮음)->함박눈(높음)
TensorFlow, Theano, CNTK(=>MS Cognitive tool kit)
원래 Keras의 구현은 이런 프레임워크들 위에 구현이 되었는데, 코딩은 바뀌지 않은 상태에서 내가 원하는 프레임워크를 선택해 구현하는 방식이었음
지금은 거의 Keras와 Tensorflow 두가지를 많이 사용하는 추세다.
Deep Learning의 구조
Layer 단위로 작업한다.
입력값에 대한 예측값이 잘 나오려면 가중치가 잘 설정되어있어야한다.
가중치의 정확한 값을 찾아내는 것이 딥러닝의 기본적인 목표
예측데이터와 원본데이터를 비교해주는 함수를 손실함수라고 한다.
Loss Function(손실 함수)를 통해 손실 점수를 알 수 있고 손실 점수를 줄이기 위해서 여러 방법이 있음.
이름이 여러가지(다 같은 말)
  - Cost Function
  - Loss Function(대표적으로 사용)
  - Error Function
  - Objective Function

Binary Crossentropy
* 둘 중 하나를 선택할 때

Categorical Crossentropy
* 여러 개 중 하나를 선택할 때

역전파(Backpropagation)
* 손실점수를 통해 다시 가중치를 역으로 설정해나가는 과정

Optimizer
손실 함수에서 나온 차이만큼 가중치를 조절(업데이트) 하는 역할
손실 점수를 최대한 줄일 때까지 반복하여 훈련한다.

* Overfitting : 한 데이터를 반복학습할수록 그 데이터에만 맞는 가중치를 갖게되어
새로운 데이터가 입력되었을 때 모델의 정확도가 떨어지게 된다.

* Underfitting : 학습횟수가 너무 적어 발생하는 현상

※ Over/Under fitting 현상이 발생하지 않도록 학습횟수를 조정하며 적절한 가중치와 손실점수, 정확도를 맞춰 최적의 모델을 찾아내는 것이 중요하다.

경사하강법
* Gradient Descent : 모든 자료를 다 검토, 속도가 느림
* SGD(Stochastic Gradient Descent) : 일반적인 경사하강법은 정직하게 값에 접근하지만 SGD는 끊어서 접근하여 값에 접근했다가 벗어났다가를 반복하여 답을 찾음.
* Momentum : 스텝 계산하여 움직인 후 관성에 따라 가는 법
* Adagrad : 안가본곳은 빠르게, 값에 가까워질수록 보폭을 줄여 세밀히 탐색
* AdaDelta : 보폭을 줄이다 멈추는 경우를 막기 위해 수정한 알고리즘 적용
* RMSProp : 맥락을 봐가며 보폭을 줄이기
* Adam : RMSProp + Momentum

epoch : 전체 학습단계 학습횟수
batch size : 학습 분할횟수


영화리뷰 자료를 활용한 인코딩 후 딥러닝을 통해 단어 횟수파악

num_words : 해당되는 단어출현빈도가 높은 것들을 정하는 수만큼 뽑아내는 옵션

정확도만 보고 접근하기보단 운영하며 생기는 변수에 따라 접근해야한다.
Tokenization